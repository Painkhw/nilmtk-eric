{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import time\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six import iteritems\n",
    "\n",
    "from nilmtk import DataSet, TimeFrame, MeterGroup, HDFDataStore\n",
    "from nilmtk.legacy.disaggregate import CombinatorialOptimisation, FHMM\n",
    "import nilmtk.utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (30, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataSet('/home/eric/Project/data/redd.h5')\n",
    "test = DataSet('/home/eric/Project/data/redd.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use building 1 for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split data at April 30th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_window(start=\"2011-04-21\",end=\"2011-05-05\")\n",
    "test.set_window(start=\"2011-05-30\")\n",
    "\n",
    "train_elec = train.buildings[1].elec\n",
    "\n",
    "test_elec = test.buildings[1].elec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_elec.mains().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDD data set has got appliance level data sampled every 3 or 4 seconds and mains data sampled every 1 second. Let us verify the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_meter = train_elec['fridge']\n",
    "microwave_meter = train_elec['microwave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_df = next(fridge_meter.load())\n",
    "microwave_df = next(microwave_meter.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_meter.plot()\n",
    "microwave_meter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains = train_elec.mains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_df = next(mains.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, both of these are sampled at different frequencies, we will downsample both to 1 minute resolution. We will also select the top-5 appliances in terms of energy consumption and use them for training our FHMM and CO models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting top-5 appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_train_elec = train_elec.submeters().select_top_k(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_train_elec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and disaggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function to disaggregate the mains data to constituent appliances and return the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, test_elec, sample_period, timezone):\n",
    "    pred = {}\n",
    "    gt= {}\n",
    "    \n",
    "    # \"ac_type\" varies according to the dataset used. \n",
    "    # Make sure to use the correct ac_type before using the default parameters in this code.    \n",
    "    for i, chunk in enumerate(test_elec.mains().load(physical_quantity = 'power', ac_type = 'apparent', sample_period=sample_period)):\n",
    "        chunk_drop_na = chunk.dropna()\n",
    "        pred[i] = clf.disaggregate_chunk(chunk_drop_na)\n",
    "        gt[i]={}\n",
    "\n",
    "        for meter in test_elec.submeters().meters:\n",
    "            # Only use the meters that we trained on (this saves time!)    \n",
    "            gt[i][meter] = next(meter.load(physical_quantity = 'power', ac_type = 'active', sample_period=sample_period))\n",
    "        gt[i] = pd.DataFrame({k:v.squeeze() for k,v in iteritems(gt[i]) if len(v)}, index=next(iter(gt[i].values())).index).dropna()\n",
    "        \n",
    "    # If everything can fit in memory\n",
    "    gt_overall = pd.concat(gt)\n",
    "    gt_overall.index = gt_overall.index.droplevel()\n",
    "    pred_overall = pd.concat(pred)\n",
    "    pred_overall.index = pred_overall.index.droplevel()\n",
    "\n",
    "    # Having the same order of columns\n",
    "    gt_overall = gt_overall[pred_overall.columns]\n",
    "    \n",
    "    #Intersection of index\n",
    "    gt_index_utc = gt_overall.index.tz_convert(\"UTC\")\n",
    "    pred_index_utc = pred_overall.index.tz_convert(\"UTC\")\n",
    "    common_index_utc = gt_index_utc.intersection(pred_index_utc)\n",
    "    \n",
    "    common_index_local = common_index_utc.tz_convert(timezone)\n",
    "    gt_overall = gt_overall.loc[common_index_local]\n",
    "    pred_overall = pred_overall.loc[common_index_local]\n",
    "    appliance_labels = [m for m in gt_overall.columns.values]\n",
    "    gt_overall.columns = appliance_labels\n",
    "    pred_overall.columns = appliance_labels\n",
    "    return gt_overall, pred_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using 2 benchmarking algorithms -  Combinatorial Optimisation (CO) and Factorial Hidden Markov Model (FHMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'CO':CombinatorialOptimisation(), 'FHMM':FHMM()}\n",
    "predictions = {}\n",
    "sample_period = 120\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(\"*\"*20)\n",
    "    print(clf_name)\n",
    "    print(\"*\" *20)\n",
    "    start = time.time()\n",
    "    # Note that we have given the sample period to downsample the data to 1 minute. \n",
    "    # If instead of top_5 we wanted to train on all appliance, we would write \n",
    "    # fhmm.train(train_elec, sample_period=60)\n",
    "    clf.train(top_5_train_elec, sample_period=sample_period)\n",
    "    end = time.time()\n",
    "    print(\"Runtime =\", end-start, \"seconds.\")\n",
    "    gt, predictions[clf_name] = predict(clf, test_elec, sample_period, train.metadata['timezone'])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using prettier labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appliance_labels = [m.label() for m in gt.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.columns = appliance_labels\n",
    "predictions['CO'].columns = appliance_labels\n",
    "predictions['FHMM'].columns = appliance_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a look at the ground truth of top 5 appliance power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions['CO'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions['FHMM'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the predictions against the actual usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions['CO']['Fridge'].head(300).plot(label=\"Pred\")\n",
    "gt['Fridge'].head(300).plot(label=\"GT\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['FHMM']['Fridge'].head(300).plot(label=\"Pred\")\n",
    "gt['Fridge'].head(300).plot(label=\"GT\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing NILM algorithms (CO vs FHMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nilmtk.utils.compute_rmse` is an extended of the following, handling both missing values and labels better:\n",
    "```python\n",
    "def compute_rmse(gt, pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    rms_error = {}\n",
    "    for appliance in gt.columns:\n",
    "        rms_error[appliance] = np.sqrt(mean_squared_error(gt[appliance], pred[appliance]))\n",
    "    return pd.Series(rms_error)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? nilmtk.utils.compute_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rmse = {}\n",
    "for clf_name in classifiers.keys():\n",
    "    rmse[clf_name] = nilmtk.utils.compute_rmse(gt, predictions[clf_name])\n",
    "\n",
    "rmse = pd.DataFrame(rmse)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}